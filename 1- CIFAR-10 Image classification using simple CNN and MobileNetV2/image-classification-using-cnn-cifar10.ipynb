{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- [Import Libraries](#1)\n- [Reading the Dataset](#2)\n- [Data Pre-processing](#3)\n- [Modeling](#4)\n    1. [Simple CNN](#8)\n    2. [Transform learning with MobileNetV2](#9)\n- [Conclusions](#5)\n- [Gradio ](#6)\n- [References](#7)","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries <a id = \"1\"></a> ","metadata":{}},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:23.660852Z","iopub.execute_input":"2022-04-22T13:14:23.661195Z","iopub.status.idle":"2022-04-22T13:14:39.199492Z","shell.execute_reply.started":"2022-04-22T13:14:23.661116Z","shell.execute_reply":"2022-04-22T13:14:39.198681Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.applications.mobilenet_v2 import preprocess_input, MobileNetV2\nfrom keras.layers import BatchNormalization, Conv2D, Dense,Dropout, Flatten, GlobalAveragePooling2D, MaxPool2D, RandomFlip, RandomRotation, ReLU, Rescaling\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:14:39.201660Z","iopub.execute_input":"2022-04-22T13:14:39.201925Z","iopub.status.idle":"2022-04-22T13:14:45.887290Z","shell.execute_reply.started":"2022-04-22T13:14:39.201891Z","shell.execute_reply":"2022-04-22T13:14:45.886560Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Dataset <a id = \"2\"></a> ","metadata":{}},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:45.888922Z","iopub.execute_input":"2022-04-22T13:14:45.889172Z","iopub.status.idle":"2022-04-22T13:14:52.185309Z","shell.execute_reply.started":"2022-04-22T13:14:45.889138Z","shell.execute_reply":"2022-04-22T13:14:52.184571Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a href = 'https://www.cs.toronto.edu/~kriz/cifar.html'>Link to CIFAR dataset.</a>\n\n<i>\"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.<br>\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\"</i>","metadata":{}},{"cell_type":"code","source":"print(f'Shape of training data: {x_train.shape}')\nprint(f'Shape of training labels: {y_train.shape}')\nprint(f'Number of training samples: {x_train.shape[0]}')\nprint(15 * '-')\nprint(f'Shape of testing data: {x_test.shape}')\nprint(f'Shape of testing labels: {y_test.shape}')\nprint(f'Number of testing samples: {x_test.shape[0]}')\nprint(15 * '-')\nprint(f'Size of images: {x_train.shape[1:4]}')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:52.187618Z","iopub.execute_input":"2022-04-22T13:14:52.187863Z","iopub.status.idle":"2022-04-22T13:14:52.196556Z","shell.execute_reply.started":"2022-04-22T13:14:52.187826Z","shell.execute_reply":"2022-04-22T13:14:52.195583Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing <a id = \"3\"></a> ","metadata":{}},{"cell_type":"code","source":"sns.countplot(np.squeeze(y_train))\nplt.xlabel('Index of each class')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:52.197903Z","iopub.execute_input":"2022-04-22T13:14:52.198187Z","iopub.status.idle":"2022-04-22T13:14:52.443607Z","shell.execute_reply.started":"2022-04-22T13:14:52.198152Z","shell.execute_reply":"2022-04-22T13:14:52.442952Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"index_to_name = {0:'Airplane', 1:'Car', 2:'Bird',\n                 3:'Cat', 4:'Deer', 5:'Dog',\n                 6:'Frog', 7:'Horse', 8:'Ship',\n                 9:'Truck'}","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:52.444891Z","iopub.execute_input":"2022-04-22T13:14:52.445123Z","iopub.status.idle":"2022-04-22T13:14:52.449626Z","shell.execute_reply.started":"2022-04-22T13:14:52.445092Z","shell.execute_reply":"2022-04-22T13:14:52.448847Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15, 9))\nfor num, i in enumerate(np.random.randint(x_train.shape[0],size = 9)):\n    plt.subplot(3,3, num + 1)\n    plt.imshow(x_train[i])\n    class_index = np.squeeze(y_train[i][0]).astype(int)\n    plt.title(f'{class_index}: {index_to_name[class_index]}')\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:14:52.451112Z","iopub.execute_input":"2022-04-22T13:14:52.451628Z","iopub.status.idle":"2022-04-22T13:14:52.846261Z","shell.execute_reply.started":"2022-04-22T13:14:52.451591Z","shell.execute_reply":"2022-04-22T13:14:52.845581Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Modeling <a id = \"4\"></a> \n","metadata":{}},{"cell_type":"markdown","source":"## Simple CNN <a id = \"8\"></a> ","metadata":{}},{"cell_type":"code","source":"def make_base_model():\n    inputs = keras.Input(shape = (32, 32, 3)) \n    x = Rescaling(1. / 255)(inputs)\n    x = RandomFlip()(x)\n    x = RandomRotation(0.2, fill_mode = 'nearest')(x)\n    x = Conv2D(filters = 8, kernel_size =  2, strides = 2)(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis = -1)(x)\n    x = MaxPool2D(pool_size = 2, strides = 2)(x)\n    x = ReLU()(x)\n    x = Conv2D(filters = 16, kernel_size =  2, strides = 2, padding = 'same')(x)\n    x = ReLU()(x)\n    x = BatchNormalization(axis = -1)(x)\n    x = MaxPool2D(pool_size = 4, strides = 4)(x)\n    x = Flatten()(x)\n    x = Dropout(0.2)(x)\n    outputs = Dense(10, activation = 'softmax')(x)\n    model = keras.Model(inputs = inputs, outputs = outputs, name = 'base_model')\n    \n    return model   \n\nbase_model = make_base_model()\nbase_model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:14:52.847269Z","iopub.execute_input":"2022-04-22T13:14:52.847637Z","iopub.status.idle":"2022-04-22T13:14:55.562927Z","shell.execute_reply.started":"2022-04-22T13:14:52.847598Z","shell.execute_reply":"2022-04-22T13:14:55.562187Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"base_model.compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = ['accuracy']\n)\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n    filepath = 'best_base_model.keras',\n    save_best_only = True,\n    monitor = 'val_accuracy')\n]\n\nepochs = 20\n\nhistory = base_model.fit(\n    x_train,\n    y_train,\n    batch_size = 32,\n    epochs = epochs,\n    shuffle = True,\n    validation_split= 0.2,\n    callbacks = callbacks\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:14:55.564035Z","iopub.execute_input":"2022-04-22T13:14:55.564278Z","iopub.status.idle":"2022-04-22T13:17:18.795389Z","shell.execute_reply.started":"2022-04-22T13:14:55.564243Z","shell.execute_reply":"2022-04-22T13:17:18.794573Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def plot_history(history_of_model):\n    plt.figure(figsize = (20, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.title('Loss on training and validation data')\n    sns.lineplot(x = range(1, epochs + 1), y = history_of_model.history['loss'])\n    sns.lineplot(x = range(1, epochs + 1), y = history_of_model.history['val_loss'])\n    plt.xlabel('epochs')\n    plt.xticks(list(range(1, epochs + 1))[::2])\n    plt.legend(['training loss', 'validation loss'], loc = 'upper left')\n    \n    plt.subplot(1, 2, 2)\n    plt.title('Accuracy on training and validation data')\n    sns.lineplot(x = range(1, epochs + 1), y = history_of_model.history['accuracy'])\n    sns.lineplot(x = range(1, epochs + 1), y = history_of_model.history['val_accuracy'])\n    plt.xlabel('epochs')\n    plt.xticks(list(range(1, epochs + 1))[::2])\n    plt.legend(['training accuracy', 'validation accuracy'], loc = 'upper left')\n    \n    plt.show() \n    \nplot_history(history)    ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:17:18.799607Z","iopub.execute_input":"2022-04-22T13:17:18.799890Z","iopub.status.idle":"2022-04-22T13:17:19.215918Z","shell.execute_reply.started":"2022-04-22T13:17:18.799853Z","shell.execute_reply":"2022-04-22T13:17:19.211873Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"best_base_model = keras.models.load_model('best_base_model.keras')\ntest_loss, test_acc = best_base_model.evaluate(x_test, y_test)\nprint(f'Accuracy on testing data with a simple CNN model: {test_acc:0.2%}')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:17:19.234758Z","iopub.execute_input":"2022-04-22T13:17:19.235293Z","iopub.status.idle":"2022-04-22T13:17:20.963048Z","shell.execute_reply.started":"2022-04-22T13:17:19.235256Z","shell.execute_reply":"2022-04-22T13:17:20.961978Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def plot_prob_pred(model, x):\n\n    plt.figure(figsize = (20, 8))\n    \n    m = x_test.shape[0]\n    \n    for num, i in enumerate(np.random.randint(m, size = 9)):\n            \n        pred_prob = model.predict(x[i][np.newaxis, ...])\n        pred_class = np.argmax(pred_prob).astype(int)\n\n        plt.subplot(3,6, 2 * num + 1)\n        plt.imshow(x_test[i])\n        plt.title(f'Prediction--> {pred_class}: {index_to_name[pred_class]}\\nGroundTruth--> {y_test[i][0]}: {index_to_name[y_test[i][0]]}')\n        plt.axis('off')\n\n        plt.subplot(3,6, 2 * num + 2)\n        bar_plot = plt.bar(x = range(10), height = list(np.squeeze(pred_prob)))\n        bar_plot[pred_class].set_color('red')\n        bar_plot[y_test[i][0]].set_color('green')\n        plt.xticks(range(10))\n        plt.ylim([0, 1])\n    \n    plt.tight_layout()\n\nplot_prob_pred(best_base_model, x_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:17:20.964734Z","iopub.execute_input":"2022-04-22T13:17:20.966738Z","iopub.status.idle":"2022-04-22T13:17:23.341162Z","shell.execute_reply.started":"2022-04-22T13:17:20.966698Z","shell.execute_reply":"2022-04-22T13:17:23.340433Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Transform learning with MobileNetV2 <a id = \"9\"></a> ","metadata":{}},{"cell_type":"code","source":"base_mobilenet = MobileNetV2(input_shape = (32, 32, 3), weights = 'imagenet', include_top = False)\n\nx_train_pr = preprocess_input(x_train)\nx_test_pr = preprocess_input(x_test)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:17:23.342118Z","iopub.execute_input":"2022-04-22T13:17:23.342365Z","iopub.status.idle":"2022-04-22T13:17:24.818633Z","shell.execute_reply.started":"2022-04-22T13:17:23.342331Z","shell.execute_reply":"2022-04-22T13:17:24.817847Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"x = base_mobilenet.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(64, activation = 'softmax')(x)\nx = Dropout(0.5)(x)\noutput_mobilenet = Dense(10, activation = 'softmax')(x)\n\nmobilenetv2 = keras.Model(inputs = base_mobilenet.inputs, outputs = output_mobilenet, name = 'base_mobilenet_model')\n\nmobilenetv2.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:17:24.820072Z","iopub.execute_input":"2022-04-22T13:17:24.820333Z","iopub.status.idle":"2022-04-22T13:17:24.932911Z","shell.execute_reply.started":"2022-04-22T13:17:24.820298Z","shell.execute_reply":"2022-04-22T13:17:24.932173Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for i, layer in enumerate(mobilenetv2.layers):\n    print(f'Layer number {i}: {layer.name}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:17:24.933971Z","iopub.execute_input":"2022-04-22T13:17:24.934417Z","iopub.status.idle":"2022-04-22T13:17:24.952587Z","shell.execute_reply.started":"2022-04-22T13:17:24.934380Z","shell.execute_reply":"2022-04-22T13:17:24.952006Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for layer in mobilenetv2.layers[:134]:\n    layer.trainable = False\n    \nfor layer in mobilenetv2.layers[134:]:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:17:24.953607Z","iopub.execute_input":"2022-04-22T13:17:24.953921Z","iopub.status.idle":"2022-04-22T13:17:24.974424Z","shell.execute_reply.started":"2022-04-22T13:17:24.953884Z","shell.execute_reply":"2022-04-22T13:17:24.973662Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"mobilenetv2.compile(\n    loss = 'sparse_categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = ['accuracy']\n)\n\ncallbacks_mobilenet = [\n    keras.callbacks.ModelCheckpoint(\n    filepath = 'best_mobilenet_model.keras',\n    save_best_only = True,\n    monitor = 'val_accuracy')\n]\n\nhistory_mobilenet = mobilenetv2.fit(\n    x_train_pr,\n    y_train,\n    batch_size = 32,\n    epochs = epochs,\n    shuffle = True,\n    validation_split= 0.2,\n    callbacks = callbacks_mobilenet\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-22T13:17:24.975908Z","iopub.execute_input":"2022-04-22T13:17:24.976719Z","iopub.status.idle":"2022-04-22T13:22:50.214458Z","shell.execute_reply.started":"2022-04-22T13:17:24.976652Z","shell.execute_reply":"2022-04-22T13:22:50.213580Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plot_history(history_mobilenet)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:22:50.217041Z","iopub.execute_input":"2022-04-22T13:22:50.217309Z","iopub.status.idle":"2022-04-22T13:22:50.584113Z","shell.execute_reply.started":"2022-04-22T13:22:50.217271Z","shell.execute_reply":"2022-04-22T13:22:50.583387Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"best_mobilenet_model = keras.models.load_model('best_mobilenet_model.keras')\ntest_mobilenet_loss, test_mobilenet_acc = best_mobilenet_model.evaluate(x_test_pr, y_test)\nprint(f'Accuracy on testing data with a transformed MobileNetV2 model: {test_mobilenet_acc:0.2%}')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:22:50.585857Z","iopub.execute_input":"2022-04-22T13:22:50.586525Z","iopub.status.idle":"2022-04-22T13:22:57.978176Z","shell.execute_reply.started":"2022-04-22T13:22:50.586488Z","shell.execute_reply":"2022-04-22T13:22:57.977216Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plot_prob_pred(best_mobilenet_model, x_test_pr)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:22:57.979973Z","iopub.execute_input":"2022-04-22T13:22:57.980254Z","iopub.status.idle":"2022-04-22T13:23:00.898061Z","shell.execute_reply.started":"2022-04-22T13:22:57.980216Z","shell.execute_reply":"2022-04-22T13:23:00.897379Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions <a id = \"5\"></a> ","metadata":{}},{"cell_type":"markdown","source":"Due to the low quality of the photos, the **optimal error rate** is assumed to be 90%.\nThe CNN model had many **avoidable biases**. This problem can be solved by increasing the model size or by changing the model architecture. Due to this, the pre-trained MobileNetV2 was used.\nAccordingly, the change in the model architecture significantly reduced **avoidable bias**.To achieve a better result, you can increase the epochs or start training from the basic layers of the mobile model. Increasing the epochs or starting from the basics of the mobilenetv2 model will allow you to achieve a better result.\n<br>\n\n<p style=\"text-align:center;\"><img src=\"https://miro.medium.com/max/1400/1*dPsfWuAvNJm29val0Uek9w.png\" width=\"450\"></p>","metadata":{}},{"cell_type":"markdown","source":"# Gradio <a id = \"6\"></a> ","metadata":{}},{"cell_type":"code","source":"def predict_with_mobilenetv2(inp):\n    inp_pr = preprocess_input(inp[np.newaxis, ...])\n    pred_prob = best_mobilenet_model.predict(inp_pr).flatten().tolist()\n    return {index_to_name[i]: pred_prob[i] for i in range(10)}","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:23:00.902122Z","iopub.execute_input":"2022-04-22T13:23:00.902628Z","iopub.status.idle":"2022-04-22T13:23:00.908342Z","shell.execute_reply.started":"2022-04-22T13:23:00.902590Z","shell.execute_reply":"2022-04-22T13:23:00.907748Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"iface = gr.Interface(fn = predict_with_mobilenetv2,\n                     inputs = gr.inputs.Image(shape = (32, 32)),\n                     outputs = gr.outputs.Label(num_top_classes = 5),\n                     live = True\n                    ).launch(share = True, debug = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T13:23:00.914634Z","iopub.execute_input":"2022-04-22T13:23:00.915359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References <a id = \"7\"></a> ","metadata":{}},{"cell_type":"markdown","source":"- <a href = 'https://towardsdatascience.com/two-important-machine-learning-concepts-to-improve-every-model-62fd058916b'>Bias and Variance: Two Important Machine Learning Concepts to Improve Every Model</a>","metadata":{}}]}